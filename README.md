# **Building All Classical NLP Models from Scratch**

This project was my personal journey to implement some classical NLP and Transformer-based models from scratch, inspired by foundational research papers. The goal was to deepen my understanding by coding these architectures .


---

The plan was to implement and refine the following papers:
1. **Attention Is All You Need** – The foundation of Transformers. (done)
2. **GPT-2** – Starting with a basic implementation, followed by enhancements. (done)
3. **RNN** - Starting by RNN basic model, followed by LSTM, GRU ...
4. **Vision Transformers (ViT)** – Adapting Transformer-based architectures for vision tasks. (done still need to upload it)
5. **Differential Transformer** – Incorporating the novel attention mechanism discussed in the referenced article.
6. **MAMBA Architecture** – An advanced architecture (details forthcoming).


