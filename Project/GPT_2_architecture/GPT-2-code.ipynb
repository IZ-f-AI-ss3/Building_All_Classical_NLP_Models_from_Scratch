{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torch import randint\n",
    "\n",
    "import os\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "\n",
    "input_path = '/Users/user/Workspace/klark-rd/projects/embedding_adaptation/notebooks/tiny_dataset_shakespear.txt'\n",
    "\n",
    "with open(input_path,'r') as f  :\n",
    "    text_dataset = f.read()\n",
    "\n",
    "num_train = int(0.8*len(text_dataset))\n",
    "train_dataset = text_dataset[:num_train]\n",
    "valid_dataset = text_dataset[num_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab =  set(text_dataset)\n",
    "char_to_id = { c:i for i, c in enumerate(vocab) }\n",
    "id_to_char = { i:c for i, c in enumerate(vocab) }\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "# encode = lambda txt : torch.Tensor([char_to_id[c] for c in txt]).long()\n",
    "encode = lambda txt : np.array([char_to_id[c] for c in txt])\n",
    "decode = lambda  ids : ''.join([id_to_char[id] for id in ids])\n",
    "encoded_train = encode(train_dataset)\n",
    "encoded_valid = encode(valid_dataset)\n",
    "\n",
    "def get_batch_of_text(encoded_dataset , block_size , batch_size) :\n",
    "\n",
    "    start_ids = torch.randint(low = 0, high = len(encoded_dataset) - block_size, size =(batch_size,) )\n",
    "\n",
    "    inputs = np.stack([encoded_dataset[start_id: start_id + block_size] for start_id in start_ids])\n",
    "    targets = np.stack([encoded_dataset[start_id + 1: start_id + block_size+1] for start_id in start_ids])\n",
    "\n",
    "    return torch.Tensor(inputs).long(), torch.Tensor(targets).long() # (batch_size, block_size) # (batch_size,) long idx\n",
    "\n",
    "# a, b = get_batch_of_text(encoded_train , block_size = 8 , batch_size = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 0., 0., 0., 0.],\n",
       "          [1., 1., 0., 0., 0.],\n",
       "          [1., 1., 1., 0., 0.],\n",
       "          [1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1.]]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(1,1,5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadAttention(nn.Module) :\n",
    "    def __init__(self , embedding_dim , nhead = 4, dropout = 0.1) : #, dim_feedforward=2048, activation = F.relu) :\n",
    "        super(MaskedMultiHeadAttention,self).__init__()\n",
    "        assert embedding_dim % nhead == 0\n",
    "        self.K_v = nn.Linear(in_features=embedding_dim , out_features=embedding_dim) # d_model --> d_model\n",
    "        self.Q_v = nn.Linear(in_features=embedding_dim , out_features=embedding_dim) # d_model --> d_model\n",
    "        self.V_v = nn.Linear(in_features= embedding_dim , out_features=embedding_dim) # d_model --> d_model\n",
    "        self.dropout =nn.Dropout(dropout)\n",
    "\n",
    "        self.d_model = embedding_dim//nhead\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.nhead = nhead\n",
    "\n",
    "        self.max_tokens = 1000\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(1,1,self.max_tokens, self.max_tokens)))\n",
    "    \n",
    "    def forward(self , x):      # ( batch_size, seq_len, embedding_dim)\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        query = self.Q_v(x).reshape(batch_size,seq_len,self.nhead, self.d_model).transpose(1,2)   # ( batch_size,  nhead ,seq_len ,  d_model)\n",
    "        key = self.K_v(x).reshape(batch_size,seq_len,self.nhead, self.d_model).transpose(1,2)    # ( batch_size,  nhead ,seq_len ,  d_model)\n",
    "        value = self.V_v(x).reshape(batch_size,seq_len,self.nhead, self.d_model).transpose(1,2)   # ( batch_size,  nhead ,seq_len ,  d_model)\n",
    "\n",
    "\n",
    "        scores = torch.matmul( query, key.transpose(-1,-2))/self.d_model**0.5    # ( batch_size, nhead,  seq_len ,  seq_len)\n",
    "\n",
    "        Mask = self.mask[:,:,:seq_len,:seq_len]\n",
    "        scores = scores.masked_fill(Mask == 0,float('-inf'))                    # ( batch_size, nhead,  seq_len ,  seq_len)\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        scores = self.dropout(scores)                                            # ( batch_size, nhead,  seq_len ,  seq_len)\n",
    "\n",
    "        result = torch.matmul(scores, value)                                     # ( batch_size, nhead,  seq_len , d_model)\n",
    "\n",
    "        # result = result.transpose(1,2).reshape(batch_size,seq_len, self.embedding_dim)              # slower and doesn't need contiguous\n",
    "        result = result.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embedding_dim) # faster and doesn't need contiguous\n",
    "\n",
    "        return result \n",
    "    \n",
    "class Block(nn.Module) :\n",
    "    def __init__(self , embedding_dim , nhead = 4, dropout = 0.1, dim_feedforward=2048,activation = nn.GELU) :\n",
    "        super(Block,self).__init__()\n",
    "        assert embedding_dim % nhead == 0\n",
    "\n",
    "        self.att_n = MaskedMultiHeadAttention(embedding_dim , nhead = nhead, dropout = dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.feed_fw = nn.Sequential(\n",
    "                            nn.Linear(embedding_dim,dim_feedforward),\n",
    "                            activation() ,\n",
    "                            nn.Linear(dim_feedforward, embedding_dim),\n",
    "                                    )\n",
    "    def forward( self, x) : #  ( batch_size, seq_len, embedding_dim)\n",
    "        x = self.layer_norm1(x) #  ( batch_size, seq_len, embedding_dim)\n",
    "        x =  x + self.att_n(x)  #  ( batch_size, seq_len, embedding_dim)\n",
    "        x = self.layer_norm2(x) #  ( batch_size, seq_len, embedding_dim)\n",
    "        x = x + self.feed_fw(x) #  ( batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        return x\n",
    "\n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self , vocab_size, embedding_dim , block_size, nhead, num_layers, dropout = 0.1 , dim_feedforward= 256, activation = nn.GELU) :\n",
    "        super(GPT2,self).__init__()\n",
    "        self.block_size =  block_size\n",
    "        self.token_embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim) # vocab_size --> embedding_dim\n",
    "        self.embedding_to_vocab = nn.Linear(embedding_dim,vocab_size)\n",
    "        self.embedding_to_vocab.weight = self.token_embedding.weight # The Same matrix for input and output\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(self.block_size, embedding_dim) )  # vocab_size --> embedding_dim\n",
    "        self.Blocks =  nn.ModuleList([Block(embedding_dim , nhead = nhead, dropout = dropout, dim_feedforward=dim_feedforward, activation = activation) for _ in range(num_layers)])\n",
    "\n",
    "\n",
    "    def forward( self , input, target = None) : # (batch_size, seq_len : long) \n",
    "        batch_size, seq_len = input.shape\n",
    "        input = self.token_embedding(input)                   # (batch_size, seq_len , embedding_dim )\n",
    "\n",
    "        positional_encoding = self.positional_encoding[ None, :seq_len, ] # (batch_size, seq_len , embedding_dim )\n",
    "        \n",
    "        input = input + positional_encoding # (batch_size, seq_len , embedding_dim )\n",
    "        for block in self.Blocks:\n",
    "            input = block(input)                             # (batch_size, seq_len , embedding_dim )\n",
    "        input = self.embedding_to_vocab(input)                # (batch_size, seq_len ,vocab_size)\n",
    "        input = F.softmax(input , dim = -1)                   # (batch_size, seq_len ,vocab_size)\n",
    "\n",
    "\n",
    "        if target is None :\n",
    "            loss = None\n",
    "        else :\n",
    "\n",
    "            input = input.view(-1, vocab_size)\n",
    "            target = target.view(-1)\n",
    "            \n",
    "            loss =  F.cross_entropy(input, target)\n",
    "\n",
    "        return input , loss                                # (batch_size, seq_len ,vocab_size) normalized\n",
    "    \n",
    "    def generate(self, idx,max_token = 300 ) : # (batch_size, seq_len) # max is block_size\n",
    "        for i in range(max_token) :\n",
    "            result,_ = self(idx[:,-self.block_size: ])    # (batch_size, seq_len , vocab_size) # max is block_size\n",
    "            # I will use only the last token to predict the next one :\n",
    "            probas = result[:,-1,:]\n",
    "            prediction = torch.multinomial(probas,1) # (batch_size, 1 )\n",
    "            idx = torch.cat((idx, prediction),dim = -1)  # add a new element\n",
    "\n",
    "        return idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs torch.Size([4, 8, 65])\n",
      "targets torch.Size([4, 8])\n",
      "inputs torch.Size([32, 65])\n",
      "targets torch.Size([32])\n",
      "loss 4.199715614318848\n",
      "generation [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Generation lllllllllllllllllllllllllllllllllllllllllllllllllll\n"
     ]
    }
   ],
   "source": [
    "block_size , batch_size = 8, 4\n",
    "model =  GPT2(vocab_size=vocab_size,embedding_dim=64,num_layers = 4 ,block_size=8,nhead=4)\n",
    "\n",
    "inputs, targets = get_batch_of_text(encoded_dataset = encoded_train , block_size = block_size , batch_size = batch_size)\n",
    "\n",
    "\n",
    "i,loss = model(inputs, targets)\n",
    "\n",
    "print('loss',loss.item())\n",
    "idx =  torch.randint(0,5, (1,1)).long()\n",
    "\n",
    "results = model.generate(idx,max_token= 50)\n",
    "generation = results[0].cpu().numpy().tolist()\n",
    "\n",
    "print('generation' , generation)\n",
    "print('Generation', decode(generation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation [1, 3, 35, 5, 43, 53, 61, 28, 53, 0, 22, 45, 26, 35, 49, 56, 50, 33, 24, 38, 22, 14]\n",
      "Generation mg'!:bYvb jtw'kRf\n",
      "OXjH\n"
     ]
    }
   ],
   "source": [
    "class GPT2_dummy(nn.Module):\n",
    "    def __init__(self , vocab_size, embedding_dim , block_size, nhead, dropout = 0.1 , dim_feedforward= 256, activation = nn.ReLU) :\n",
    "        super(GPT2_dummy,self).__init__()\n",
    "        self.block_size =  block_size\n",
    "        self.token_embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=vocab_size) # vocab_size --> embedding_dim\n",
    "    \n",
    "\n",
    "    def forward( self , input, target = None) : # (batch_size, seq_len : long) \n",
    "        batch_size, seq_len = input.shape\n",
    "        input = self.token_embedding(input)                   # (batch_size, seq_len , embedding_dim )\n",
    "        input = F.softmax(input , dim = -1)              \n",
    "    \n",
    "        if target is None :\n",
    "            loss = None\n",
    "        else :\n",
    "            loss =  F.cross_entropy(input, target)\n",
    "\n",
    "        return input , loss                                # (batch_size, seq_len ,vocab_size) normalized\n",
    "    \n",
    "    def generate(self, idx, max_token = 1 ) : # (batch_size, seq_len) # max is block_size\n",
    "        for i in range(max_token) :\n",
    "            result,_ = self(idx[:,-self.block_size: ])    # (batch_size, seq_len , vocab_size) # max is block_size\n",
    "            # I will use only the last token to predict the next one :\n",
    "            probas = result[:,-1,:]\n",
    "            prediction = torch.multinomial(probas,1) # (batch_size, )\n",
    "            idx = torch.cat((idx, prediction), dim = -1)  # add a new element\n",
    "\n",
    "        return idx # (batch_size, max_token) # ids long\n",
    "model =  GPT2_dummy(vocab_size=vocab_size,embedding_dim=64 ,block_size=8,nhead=4)\n",
    "tensor1 = torch.randint(0,32, (4,6))\n",
    "\n",
    "i,l = model(tensor1)\n",
    "\n",
    "idx =  torch.randint(0,5, (1,2)).long()\n",
    "\n",
    "results = model.generate(idx,max_token= 20)\n",
    "generation = results[0].cpu().numpy().tolist()\n",
    "\n",
    "print('generation' , generation)\n",
    "print('Generation', decode(generation))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
